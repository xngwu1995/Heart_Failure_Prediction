{"cells":[{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:19:32.324928Z","iopub.status.busy":"2023-05-02T18:19:32.324228Z","iopub.status.idle":"2023-05-02T18:20:05.277729Z","shell.execute_reply":"2023-05-02T18:20:05.276220Z","shell.execute_reply.started":"2023-05-02T18:19:32.324866Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","import gensim\n","import tensorflow as tf\n","import random\n","\n","from datetime import timedelta\n","from nltk.corpus import stopwords\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n","from sklearn.utils import resample\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.feature_selection import chi2\n","from sklearn.metrics import classification_report\n","\n","\n","# Setup seed\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","tf.random.set_seed(SEED)\n","\n","# Load pre-trained PubMed word2vec vectors\n","w2v_model = gensim.models.KeyedVectors.load_word2vec_format(\"/kaggle/input/pubmed-w2v/PubMed-w2v.bin\", binary=True)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:20:05.280906Z","iopub.status.busy":"2023-05-02T18:20:05.280467Z","iopub.status.idle":"2023-05-02T18:21:28.494038Z","shell.execute_reply":"2023-05-02T18:21:28.492755Z","shell.execute_reply.started":"2023-05-02T18:20:05.280862Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n","  exec(code_obj, self.user_global_ns, self.user_ns)\n"]}],"source":["# Load the data\n","admissions = pd.read_csv('/kaggle/input/cs598-heart-failure-data/ADMISSIONS.csv')\n","notes = pd.read_csv('/kaggle/input/cs598-heart-failure-data/NOTEEVENTS.csv')\n","diag = pd.read_csv('/kaggle/input/cs598-heart-failure-data/DIAGNOSES_ICD.csv')"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:21:28.496959Z","iopub.status.busy":"2023-05-02T18:21:28.496425Z","iopub.status.idle":"2023-05-02T18:21:31.535728Z","shell.execute_reply":"2023-05-02T18:21:31.534498Z","shell.execute_reply.started":"2023-05-02T18:21:28.496907Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  if sys.path[0] == \"\":\n"]}],"source":["# List of ICD-9 codes for heart failure\n","heart_failure_codes = ['39891', '40201', '40211', '40291', '40401', '40403', '40411', '40413', '40491', '40493', '4280', '4281', '42820', '42821', '42822', '42823', '42830', '42831', '42832', '42833', '42840', '42841', '42842', '42843', '4289']\n","\n","# Filter heart failure admissions based on ICD-9 codes\n","heart_failure_admissions = diag[diag['ICD9_CODE'].isin(heart_failure_codes)]\n","\n","# Merge heart failure admissions with notes and filter discharge summaries\n","hf_admissions_notes = heart_failure_admissions.merge(notes, on=['HADM_ID', 'SUBJECT_ID'])\n","hf_admissions_discharge_summaries = hf_admissions_notes[hf_admissions_notes['CATEGORY'] == 'Discharge summary']\n","\n","# Keep only the longest discharge summary for each admission\n","hf_admissions_discharge_summaries['TEXT_LENGTH'] = hf_admissions_discharge_summaries['TEXT'].str.len()\n","hf_admissions_discharge_summaries = hf_admissions_discharge_summaries.loc[hf_admissions_discharge_summaries.groupby('HADM_ID')['TEXT_LENGTH'].idxmax()]"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:21:31.538376Z","iopub.status.busy":"2023-05-02T18:21:31.537926Z","iopub.status.idle":"2023-05-02T18:21:31.604566Z","shell.execute_reply":"2023-05-02T18:21:31.603105Z","shell.execute_reply.started":"2023-05-02T18:21:31.538329Z"},"trusted":true},"outputs":[],"source":["hf_admissions_discharge_summaries = pd.merge(hf_admissions_discharge_summaries, admissions[['HADM_ID', 'ADMITTIME', 'DISCHTIME']], on='HADM_ID')\n","hf_admissions = pd.merge(heart_failure_admissions, admissions[['HADM_ID', 'ADMITTIME', 'DISCHTIME']], on='HADM_ID')"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:21:31.608222Z","iopub.status.busy":"2023-05-02T18:21:31.607882Z","iopub.status.idle":"2023-05-02T18:21:38.146487Z","shell.execute_reply":"2023-05-02T18:21:38.144853Z","shell.execute_reply.started":"2023-05-02T18:21:31.608188Z"},"trusted":true},"outputs":[],"source":["# Function to label general readmissions and 30-day readmissions\n","def label_readmissions(df):\n","    df['ADMITTIME'] = pd.to_datetime(df['ADMITTIME'])\n","    df['DISCHTIME'] = pd.to_datetime(df['DISCHTIME'])\n","    df.sort_values(['SUBJECT_ID', 'ADMITTIME'], inplace=True)\n","    df['READMISSION_TIME'] = df.groupby('SUBJECT_ID')['ADMITTIME'].diff().dt.total_seconds().div(60)\n","\n","    df['GENERAL_READMISSION'] = df['READMISSION_TIME'].apply(lambda x: 1 if x > 0 else 0)\n","    df['30_DAY_READMISSION'] = df['READMISSION_TIME'].apply(lambda x: 1 if 0 < x <= 30 * 24 * 60 else 0)\n","    \n","    return df\n","\n","# B. Labeling Readmissions (with discharge summaries)\n","hf_admissions_discharge_summaries = label_readmissions(hf_admissions_discharge_summaries)\n","\n","# B. Labeling Readmissions (without discharge summaries)\n","hf_admissions = label_readmissions(hf_admissions)\n","\n","\n"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:21:38.149238Z","iopub.status.busy":"2023-05-02T18:21:38.148571Z","iopub.status.idle":"2023-05-02T18:21:38.216936Z","shell.execute_reply":"2023-05-02T18:21:38.215375Z","shell.execute_reply.started":"2023-05-02T18:21:38.149195Z"},"trusted":true},"outputs":[],"source":["def split_train_test_dataset(df):\n","    # Separate positive and negative samples\n","    general_readmission_pos = df[df['GENERAL_READMISSION'] == 1]\n","    general_readmission_neg = df[df['GENERAL_READMISSION'] == 0]\n","    thirty_day_readmission_pos = df[df['30_DAY_READMISSION'] == 1]\n","    thirty_day_readmission_neg = df[df['30_DAY_READMISSION'] == 0]\n","\n","    # Perform under-sampling\n","    general_readmission_neg = general_readmission_neg.sample(len(general_readmission_pos))\n","    thirty_day_readmission_neg = thirty_day_readmission_neg.sample(len(thirty_day_readmission_pos))\n","\n","    # Combine and shuffle the balanced samples\n","    general_readmission_data = pd.concat([general_readmission_pos, general_readmission_neg]).sample(frac=1).reset_index(drop=True)\n","    thirty_day_readmission_data = pd.concat([thirty_day_readmission_pos, thirty_day_readmission_neg]).sample(frac=1).reset_index(drop=True)\n","\n","    # Split the data into training and test sets (90-10)\n","    train_general, test_general = train_test_split(general_readmission_data, test_size=0.1, random_state=SEED)\n","    train_thirty_day, test_thirty_day = train_test_split(thirty_day_readmission_data, test_size=0.1, random_state=SEED)\n","    \n","    # Perform 10-fold cross-validation on the training data\n","    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n","    train_general_cv_splits = list(skf.split(train_general.drop('GENERAL_READMISSION', axis=1), train_general['GENERAL_READMISSION']))\n","    train_thirty_day_cv_splits = list(skf.split(train_thirty_day.drop('30_DAY_READMISSION', axis=1), train_thirty_day['30_DAY_READMISSION']))\n","    \n","    return train_general, test_general, train_thirty_day, test_thirty_day, train_general_cv_splits, train_thirty_day_cv_splits\n","\n","train_general_ds, test_general_ds, train_thirty_day_ds, test_thirty_day_ds, train_general_ds_cv_splits, train_thirty_day_ds_cv_splits = split_train_test_dataset(hf_admissions_discharge_summaries)\n"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:21:38.219669Z","iopub.status.busy":"2023-05-02T18:21:38.219280Z","iopub.status.idle":"2023-05-02T18:21:38.228845Z","shell.execute_reply":"2023-05-02T18:21:38.227309Z","shell.execute_reply.started":"2023-05-02T18:21:38.219629Z"},"trusted":true},"outputs":[],"source":["with open(\"/kaggle/input/nltk-data/nltk_data/corpora/stopwords/english\", \"r\") as f:\n","    stop_words = [line.strip() for line in f]"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:21:38.231300Z","iopub.status.busy":"2023-05-02T18:21:38.230930Z","iopub.status.idle":"2023-05-02T18:25:10.921545Z","shell.execute_reply":"2023-05-02T18:25:10.918696Z","shell.execute_reply.started":"2023-05-02T18:21:38.231260Z"},"trusted":true},"outputs":[],"source":["## Preprocessing the text: tokenize first,then remove stopwords and non-alphabetic tokens in text(like numbers), and re-join all tokens into the text\n","\n","def preprocess_text(text):\n","    tokens = nltk.word_tokenize(text)\n","    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n","\n","    return \" \".join(tokens)\n","\n","# Preprocess text for general readmission\n","train_general_ds[\"TEXT\"] = train_general_ds[\"TEXT\"].apply(preprocess_text)\n","test_general_ds[\"TEXT\"] = test_general_ds[\"TEXT\"].apply(preprocess_text)\n","\n","# Preprocess text for 30-day readmission\n","train_thirty_day_ds[\"TEXT\"] = train_thirty_day_ds[\"TEXT\"].apply(preprocess_text)\n","test_thirty_day_ds[\"TEXT\"] = test_thirty_day_ds[\"TEXT\"].apply(preprocess_text)\n"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:25:10.927008Z","iopub.status.busy":"2023-05-02T18:25:10.926401Z","iopub.status.idle":"2023-05-02T18:25:21.868156Z","shell.execute_reply":"2023-05-02T18:25:21.866863Z","shell.execute_reply.started":"2023-05-02T18:25:10.926948Z"},"trusted":true},"outputs":[],"source":["# Tokenize words in text and padding each text to max_length\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(train_general_ds[\"TEXT\"]) # fit_on_texts creates the vocabulary index based on word frequency. \n","vocab_size = len(tokenizer.word_index) + 1\n","max_length = 256\n","\n","X_train = pad_sequences(tokenizer.texts_to_sequences(train_general_ds[\"TEXT\"]), maxlen=max_length)\n","X_test = pad_sequences(tokenizer.texts_to_sequences(test_general_ds[\"TEXT\"]), maxlen=max_length)\n","\n","y_train = train_general_ds[\"GENERAL_READMISSION\"]\n","y_test = test_general_ds[\"GENERAL_READMISSION\"]\n","\n","# Tokenize and pad sequences for 30-day readmission\n","X_train_30d = pad_sequences(tokenizer.texts_to_sequences(train_thirty_day_ds[\"TEXT\"]), maxlen=max_length)\n","X_test_30d = pad_sequences(tokenizer.texts_to_sequences(test_thirty_day_ds[\"TEXT\"]), maxlen=max_length)\n","\n","y_train_30d = train_thirty_day_ds[\"30_DAY_READMISSION\"]\n","y_test_30d = test_thirty_day_ds[\"30_DAY_READMISSION\"]\n"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:25:21.870038Z","iopub.status.busy":"2023-05-02T18:25:21.869633Z","iopub.status.idle":"2023-05-02T18:25:21.875726Z","shell.execute_reply":"2023-05-02T18:25:21.874113Z","shell.execute_reply.started":"2023-05-02T18:25:21.870002Z"},"trusted":true},"outputs":[],"source":["# Create a weight matrix for words in the training set\n","embedding_dim = 200  # Change this to the dimension of the pre-trained vectors in word2vec\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","for word, index in tokenizer.word_index.items():\n","    if word in w2v_model:\n","        embedding_vector = w2v_model[word]\n","        if embedding_vector is not None:\n","            embedding_matrix[index] = embedding_vector"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:25:21.877591Z","iopub.status.busy":"2023-05-02T18:25:21.877263Z","iopub.status.idle":"2023-05-02T18:25:21.895366Z","shell.execute_reply":"2023-05-02T18:25:21.893874Z","shell.execute_reply.started":"2023-05-02T18:25:21.877559Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.layers import MaxPooling1D, Dropout, Concatenate, Input\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.models import Model\n","from sklearn.metrics import classification_report\n","\n","# Define the CNN model\n","def create_model():\n","    inputs = Input(shape=(max_length,))\n","    \n","    embedded_sequences = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=True)(inputs)\n","\n","    conv_outputs = []\n","    for filter_size in [1, 2, 3]:\n","        conv = Conv1D(128, filter_size, activation='relu', kernel_regularizer=l2(0.001))(embedded_sequences)\n","        conv = MaxPooling1D(pool_size=2)(conv)\n","        conv_outputs.append(conv)\n","\n","    concatenated = Concatenate(axis=1)(conv_outputs)\n","\n","    conv1 = Conv1D(64, 5, activation='relu', kernel_regularizer=l2(0.001))(concatenated)\n","    conv1 = MaxPooling1D(pool_size=2)(conv1)\n","    \n","    conv2 = Conv1D(32, 5, activation='relu', kernel_regularizer=l2(0.001))(conv1)\n","    pooled = GlobalMaxPooling1D()(conv2)\n","    \n","    dense1 = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(pooled)\n","    dropout1 = Dropout(0.6)(dense1)\n","    \n","    dense2 = Dense(32, activation='relu', kernel_regularizer=l2(0.001))(dropout1)\n","    dropout2 = Dropout(0.6)(dense2)\n","    \n","    outputs = Dense(1, activation='sigmoid')(dropout2)\n","\n","    model = Model(inputs=inputs, outputs=outputs)\n","\n","    # Compile the model\n","    optimizer = Adam(lr=0.0005)\n","    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","# Set up the callbacks\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.0001)\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:25:21.897584Z","iopub.status.busy":"2023-05-02T18:25:21.897232Z","iopub.status.idle":"2023-05-02T18:25:21.910993Z","shell.execute_reply":"2023-05-02T18:25:21.909853Z","shell.execute_reply.started":"2023-05-02T18:25:21.897549Z"},"trusted":true},"outputs":[],"source":["accuracy_general_cv = []\n","precision_general_cv = []\n","recall_general_cv = []\n","f1_general_cv = []\n","\n","for train_index, val_index in train_general_ds_cv_splits:\n","    X_train_cv = X_train[train_index]\n","    X_val_cv = X_train[val_index]\n","    y_train_cv = y_train.values[train_index]\n","    y_val_cv = y_train.values[val_index]\n","\n","    model_general_cv = create_model()\n","\n","    history_general_cv = model_general_cv.fit(\n","        X_train_cv, y_train_cv,\n","        epochs=20,\n","        batch_size=40,\n","        validation_data=(X_val_cv, y_val_cv),\n","        callbacks=[reduce_lr, early_stopping]\n","    )\n","\n","    y_pred_general_cv = model_general_cv.predict(X_val_cv)\n","    y_pred_general_cv = [1 if p >= 0.5 else 0 for p in y_pred_general_cv]\n","\n","    report_general_cv = classification_report(y_val_cv, y_pred_general_cv, output_dict=True)\n","\n","    accuracy_general_cv.append(report_general_cv['accuracy'])\n","    precision_general_cv.append(report_general_cv['1']['precision'])\n","    recall_general_cv.append(report_general_cv['1']['recall'])\n","    f1_general_cv.append(report_general_cv['1']['f1-score'])\n","\n","print(f\"General readmission - Mean CV accuracy: {np.mean(accuracy_general_cv):.4f}\")\n","print(f\"General readmission - Mean CV precision: {np.mean(precision_general_cv):.4f}\")\n","print(f\"General readmission - Mean CV recall: {np.mean(recall_general_cv):.4f}\")\n","print(f\"General readmission - Mean CV F1 score: {np.mean(f1_general_cv):.4f}\")\n"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:25:21.913234Z","iopub.status.busy":"2023-05-02T18:25:21.912780Z","iopub.status.idle":"2023-05-02T18:25:21.926362Z","shell.execute_reply":"2023-05-02T18:25:21.925204Z","shell.execute_reply.started":"2023-05-02T18:25:21.913187Z"},"trusted":true},"outputs":[],"source":["# Create, train, and evaluate the model for 30-day readmission using 10-fold cross-validation\n","accuracy_30d_cv = []\n","precision_30d_cv = []\n","recall_30d_cv = []\n","f1_30d_cv = []\n","\n","for train_index, val_index in train_thirty_day_ds_cv_splits:\n","    X_train_30d_cv = X_train_30d[train_index]\n","    X_val_30d_cv = X_train_30d[val_index]\n","    y_train_30d_cv = y_train_30d.values[train_index]\n","    y_val_30d_cv = y_train_30d.values[val_index]\n","\n","    model_30d_cv = create_model()\n","\n","    history_30d_cv = model_30d_cv.fit(\n","        X_train_30d_cv, y_train_30d_cv,\n","        epochs=20,\n","        batch_size=45,\n","        validation_data=(X_val_30d_cv, y_val_30d_cv),\n","        callbacks=[reduce_lr, early_stopping]\n","    )\n","\n","    y_pred_30d_cv = model_30d_cv.predict(X_val_30d_cv)\n","    y_pred_30d_cv = [1 if p >= 0.5 else 0 for p in y_pred_30d_cv]\n","\n","    report_30d_cv = classification_report(y_val_30d_cv, y_pred_30d_cv, output_dict=True)\n","\n","    accuracy_30d_cv.append(report_30d_cv['accuracy'])\n","    precision_30d_cv.append(report_30d_cv['1']['precision'])\n","    recall_30d_cv.append(report_30d_cv['1']['recall'])\n","    f1_30d_cv.append(report_30d_cv['1']['f1-score'])\n","\n","print(f\"30-day readmission - Mean CV accuracy: {np.mean(accuracy_30d_cv):.4f}\")\n","print(f\"30-day readmission - Mean CV precision: {np.mean(precision_30d_cv):.4f}\")\n","print(f\"30-day readmission - Mean CV recall: {np.mean(recall_30d_cv):.4f}\")\n","print(f\"30-day readmission - Mean CV F1 score: {np.mean(f1_30d_cv):.4f}\")\n"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:25:21.931210Z","iopub.status.busy":"2023-05-02T18:25:21.930243Z","iopub.status.idle":"2023-05-02T18:25:21.943065Z","shell.execute_reply":"2023-05-02T18:25:21.941816Z","shell.execute_reply.started":"2023-05-02T18:25:21.931156Z"},"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_selection import chi2\n","\n","reverse_word_index = {index: word for word, index in tokenizer.word_index.items()}\n","\n","def revert_to_text(encoded_text):\n","    return ' '.join([reverse_word_index[index] for index in encoded_text if index != 0])\n","\n","X_train_text = [revert_to_text(encoded_text) for encoded_text in X_train]\n","\n","vectorizer = CountVectorizer(tokenizer=nltk.word_tokenize, stop_words='english')\n","X_chi2 = vectorizer.fit_transform(X_train_text)\n","\n","# For general readmission\n","y_general_chi2 = train_general_ds['GENERAL_READMISSION']\n","chi2_general_score, p_general_value = chi2(X_chi2, y_general_chi2)\n","\n","# For 30-day readmission\n","y_30day_chi2 = train_general_ds['30_DAY_READMISSION']\n","chi2_30day_score, p_30day_value = chi2(X_chi2, y_30day_chi2)\n","\n","feature_names = vectorizer.get_feature_names()\n","\n","# Top 20 words for general readmission\n","chi2_general_sorted_indices = chi2_general_score.argsort()[::-1]\n","top_20_general_words = [feature_names[i] for i in chi2_general_sorted_indices[:20]]\n","\n","# Top 20 words for 30-day readmission\n","chi2_30day_sorted_indices = chi2_30day_score.argsort()[::-1]\n","top_20_30day_words = [feature_names[i] for i in chi2_30day_sorted_indices[:20]]\n","\n","print(\"Top 20 words related to general readmission:\")\n","print(top_20_general_words)\n","\n","print(\"\\nTop 20 words related to 30-day readmission:\")\n","print(top_20_30day_words)\n"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:47:36.746276Z","iopub.status.busy":"2023-05-02T18:47:36.744789Z","iopub.status.idle":"2023-05-02T18:48:02.174888Z","shell.execute_reply":"2023-05-02T18:48:02.173504Z","shell.execute_reply.started":"2023-05-02T18:47:36.746221Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["General readmission - Accuracy Score: 0.6847360912981455\n","General readmission - Precision Score: 0.6854072413558862\n","General readmission - Recall Score: 0.6847360912981455\n","General readmission - F1 Score: 0.6848362042120734\n"]}],"source":["## Random Forest model\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n","\n","# Extract features from clinical notes using TF-IDF vectorizer for general readmission\n","vectorizer = TfidfVectorizer(analyzer='word')\n","X_train = vectorizer.fit_transform(train_general_ds[\"TEXT\"]).toarray()\n","X_test = vectorizer.transform(test_general_ds[\"TEXT\"]).toarray()\n","\n","y_train = train_general_ds[\"GENERAL_READMISSION\"]\n","y_test = test_general_ds[\"GENERAL_READMISSION\"]\n","\n","# Train the Random Forest model for general readmission\n","model = RandomForestClassifier(n_estimators=40, random_state=42)\n","model.fit(X_train, y_train)\n","\n","# predict on test data for general readmission\n","y_pred = model.predict(X_test)\n","\n","\n","print(f\"General readmission - Accuracy Score: {accuracy_score(y_test, y_pred)}\")\n","print(f\"General readmission - Precision Score: {precision_score(y_test, y_pred,average='weighted')}\")\n","print(f\"General readmission - Recall Score: {recall_score(y_test, y_pred,average='weighted')}\")\n","print(f\"General readmission - F1 Score: {f1_score(y_test, y_pred,average='weighted')}\")\n","\n","\n"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2023-05-02T18:34:32.844550Z","iopub.status.busy":"2023-05-02T18:34:32.843869Z","iopub.status.idle":"2023-05-02T18:34:46.169896Z","shell.execute_reply":"2023-05-02T18:34:46.168411Z","shell.execute_reply.started":"2023-05-02T18:34:32.844508Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["30-day readmission - Accuracy Score: 0.5378031383737518\n","30-day readmission - Precision Score: 0.8619966243932007\n","30-day readmission - Recall Score: 0.5378031383737518\n","30-day readmission - F1 Score: 0.6274822207709208\n"]}],"source":["# Extract features from clinical notes using TF-IDF vectorizer for 30-day readmission\n","vectorizer_30 = TfidfVectorizer(analyzer='word')\n","X_train_30 = vectorizer_30.fit_transform(train_thirty_day_ds[\"TEXT\"]).toarray()\n","X_test_30 = vectorizer_30.transform(test_thirty_day_ds[\"TEXT\"]).toarray()\n","\n","y_train_30 = train_general_ds[\"30_DAY_READMISSION\"]\n","y_test_30 = test_general_ds[\"30_DAY_READMISSION\"]\n","\n","# Train the Random Forest model for 30-day readmission\n","model_30 = RandomForestClassifier(n_estimators=40, random_state=100)\n","model_30.fit(X_train, y_train)\n","\n","# predict on test data for 30-day readmission\n","y_pred_30 = model.predict(X_test)\n","\n","print(f\"30-day readmission - Accuracy Score: {accuracy_score(y_test_30, y_pred_30)}\")\n","print(f\"30-day readmission - Precision Score: {precision_score(y_test_30, y_pred_30,average='weighted')}\")\n","print(f\"30-day readmission - Recall Score: {recall_score(y_test_30, y_pred_30,average='weighted')}\")\n","print(f\"30-day readmission - F1 Score: {f1_score(y_test_30, y_pred_30,average='weighted')}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
